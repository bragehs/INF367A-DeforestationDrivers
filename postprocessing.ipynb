{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "#1. Add the 64*64 outputs to one image.\n",
    "#2. For every pixel, convert it into the most probable one.Maybe depend on neighbours??\n",
    "# Could be multiple overlaps, find a way to handle that.\n",
    "#3. Convert every pixel to correct polygon json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import json\n",
    "#import satlaspretrain_models\n",
    "import torch\n",
    "from scipy.signal import convolve2d\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_optimized(raw_output, patch_size):\n",
    "    \"\"\"\n",
    "    Post-processes the raw output from the model using convolution for faster accumulation,\n",
    "    and then determines class labels using argmax.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_output, np.ndarray):\n",
    "        raise TypeError(\"Input must be a NumPy array.\")\n",
    "    if raw_output.shape != (1024, 1024, 5):\n",
    "        raise ValueError(\"Input must have shape (1024, 1024, 5).\")\n",
    "\n",
    "    kernel = np.ones((patch_size, patch_size), dtype=np.float32).reshape(patch_size, patch_size, 1)\n",
    "    # Use ndimage.convolve once for all channels\n",
    "    accumulated_probs = ndimage.convolve(raw_output, kernel, mode='constant', cval=0)\n",
    "\n",
    "    class_labels = np.argmax(accumulated_probs, axis=-1)\n",
    "    return class_labels\n",
    "\n",
    "def test_post_process_optimized():\n",
    "    positions = np.mgrid[0:1024:256, 0:1024:256].reshape(2, -1).T\n",
    "    \n",
    "    probs = np.random.dirichlet(np.ones(5)*2, size=positions.shape[0])\n",
    "    probs[:,0] = np.clip(probs[:,0], 0.3, 1.0)\n",
    "    probs = probs / probs.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    outputs = [(x, y, p.tolist()) for (x, y), p in zip(positions, probs)]\n",
    "    print((outputs[0]))\n",
    "    return post_process_optimized(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('reconstructed_image.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024, 1024, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_classes = np.argmax(predictions, axis=-1)\n",
    "raw_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m assigned_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpost_process_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mpost_process_optimized\u001b[0;34m(raw_output, patch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m kernel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((patch_size, patch_size), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mreshape(patch_size, patch_size, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Use ndimage.convolve once for all channels\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m accumulated_probs \u001b[38;5;241m=\u001b[39m \u001b[43mndimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconstant\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(accumulated_probs, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m class_labels\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opencv_env/lib/python3.8/site-packages/scipy/ndimage/_filters.py:890\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(input, weights, output, mode, cval, origin)\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;129m@_ni_docstrings\u001b[39m\u001b[38;5;241m.\u001b[39mdocfiller\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvolve\u001b[39m(\u001b[38;5;28minput\u001b[39m, weights, output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m, cval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    785\u001b[0m              origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m    786\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;124;03m    Multidimensional convolution.\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    888\u001b[0m \n\u001b[1;32m    889\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 890\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_correlate_or_convolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43morigin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opencv_env/lib/python3.8/site-packages/scipy/ndimage/_filters.py:712\u001b[0m, in \u001b[0;36m_correlate_or_convolve\u001b[0;34m(input, weights, output, mode, cval, origin, convolution)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sequence of modes is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    711\u001b[0m mode \u001b[38;5;241m=\u001b[39m _ni_support\u001b[38;5;241m.\u001b[39m_extend_mode_to_code(mode)\n\u001b[0;32m--> 712\u001b[0m \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temp_needed:\n\u001b[1;32m    714\u001b[0m     temp[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assigned_labels = post_process_optimized(predictions, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 3, 3, 3],\n",
       "       [0, 0, 0, ..., 3, 3, 3],\n",
       "       [0, 0, 0, ..., 3, 3, 3],\n",
       "       ...,\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2],\n",
       "       [2, 2, 2, ..., 2, 2, 2]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assigned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch and cuda version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def post_process_torch(outputs, gamma=0.5):\n",
    "    device = torch.device(\"cpu\")  # Assuming model outputs are on CUDA\n",
    "    empty_img = torch.zeros((1024, 1024, 5), device=device)\n",
    "    \n",
    "    # Convert outputs to tensor once\n",
    "    coords = torch.tensor([[x,y] for x,y,_ in outputs], device=device)\n",
    "    probs = torch.tensor([p for _,_,p in outputs], device=device)\n",
    "    \n",
    "    # Batch assignment\n",
    "    for coord, prob in zip(coords, probs):\n",
    "        empty_img[coord[0]:coord[0]+64, coord[1]:coord[1]+64] += prob\n",
    "    \n",
    "    padded = torch.nn.functional.pad(empty_img, (0,0,1,1,1,1))\n",
    "    neighbors = gamma * (padded[:-2, 1:-1] + padded[2:, 1:-1] + \n",
    "                        padded[1:-1, :-2] + padded[1:-1, 2:])\n",
    "    \n",
    "    return torch.argmax(empty_img + neighbors, dim=2)  # Keep on GPU if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From pixel img to polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def converter(tensor, size_limit=1000):\n",
    "    # Dictionary to store polygons for each unique value\n",
    "    polygons = {}\n",
    "    # Ensure tensor is numpy and squeeze to remove batch dimension\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        tensor = tensor.squeeze().numpy().astype(np.uint8)\n",
    "    \n",
    "    for val in range(5):  # Iterate over unique values (0-4)\n",
    "        mask = np.uint8((tensor == val) * 255)  # Create binary mask\n",
    "        \n",
    "        # Find contours (polygons) of connected components\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        # Filter out small connected components\n",
    "        large_contours = []\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area >= size_limit:  # Keep only contours with area >= size_limit\n",
    "                large_contours.append(contour)\n",
    "        \n",
    "        # Create a new mask and draw the large contours\n",
    "        cleaned_mask = np.zeros_like(mask)\n",
    "        cv2.drawContours(cleaned_mask, large_contours, -1, 255, thickness=cv2.FILLED)\n",
    "        \n",
    "        # Update the original mask\n",
    "        tensor[tensor == val] = 0  # Clear current value in the original mask\n",
    "        tensor[cleaned_mask == 255] = val  # Add cleaned mask\n",
    "        \n",
    "        # Convert contours to a list of polygons\n",
    "        polygons[val] = [c.reshape(-1, 2).tolist() for c in large_contours]  # Use filtered contours\n",
    "    \n",
    "    return polygons\n",
    "\n",
    "# Test tensor with one square of 1s in top left corner of size 50x50\n",
    "# Rest of the tensor is zero\n",
    "test_tensor = torch.zeros((1, 1024, 1024))\n",
    "# Create a square of 1s\n",
    "test_tensor[0, 0:120, 0:120] = 1\n",
    "\n",
    "# Apply the converter with a large size_limit\n",
    "#final_prediction = converter(tensor=test_tensor, size_limit=10000)\n",
    "final_prediction = converter(assigned_labels)\n",
    "\n",
    "\n",
    "def write_json(polygons, filename:str):\n",
    "    #Function that takes list of polygons and writes them to a json file.\n",
    "    #The list of polygons is a list of lists of polygons, where each polygon is a list of points.\n",
    "    # Dictionary to map unique values to class names\n",
    "    numper_2_class={1:\"plantation\",2:\"grassland_shrubland\",3:\"mining\",4:\"logging\"}\n",
    "    with open(f\"{filename}.json\", \"w\") as file:\n",
    "        images_overview={\"images\":[]}\n",
    "        # Iterate over images in polygons\n",
    "        for i in range(len(polygons)):\n",
    "\n",
    "            curr={\"file_name\":f\"evaluation_{i}.tif\",\"annotations\":[]}\n",
    "            #Iterate over types of polygons in image\n",
    "            for j in range(1,len(polygons[i])):\n",
    "                # Convert polygons to list of coordinates\n",
    "\n",
    "                for k in range(len(polygons[i][j])):\n",
    "                    listed_polygons=[]\n",
    "\n",
    "                    for p in range(len(polygons[i][j][k])):\n",
    "                        listed_polygons.append(polygons[i][j][k][p][0])\n",
    "                        listed_polygons.append(polygons[i][j][k][p][1])\n",
    "                    curr[\"annotations\"].append({\"class\":numper_2_class[j],\"segmentation\":listed_polygons})\n",
    "            images_overview[\"images\"].append(curr)\n",
    "        file.write(json.dumps(images_overview, ensure_ascii=False, indent=4))\n",
    "write_json([final_prediction],\"test\")\n",
    "# Now, polygons[val] contains a list of polygons for each unique value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterized image saved to rasterized_prediction.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Assuming y_test is a numpy array or torch tensor containing the\n",
    "# class labels for the entire 1024x1024 image.\n",
    "# Example:\n",
    "# y_test = ... # Your test labels (numpy array or torch tensor)\n",
    "\n",
    "# If y_test is a torch tensor, convert it to a numpy array\n",
    "if isinstance(assigned_labels, torch.Tensor):\n",
    "    assigned_labels = assigned_labels.cpu().numpy()  # Move to CPU if it's on GPU\n",
    "\n",
    "# Ensure y_test is of integer type\n",
    "assigned_labels = assigned_labels.astype(np.uint8)\n",
    "\n",
    "# Create a color mapping for the classes\n",
    "color_map = {\n",
    "    0: [0, 0, 0],      # background: black\n",
    "    1: [255, 0, 0],    # plantation: red\n",
    "    2: [0, 255, 0],    # grassland_shrubland: green\n",
    "    3: [0, 0, 255],    # mining: blue\n",
    "    4: [255, 255, 0]     # logging: yellow\n",
    "}\n",
    "\n",
    "# Create an RGB image where each class is represented by its color\n",
    "height, width = assigned_labels.shape\n",
    "rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        class_id = assigned_labels[i, j]\n",
    "        rgb_image[i, j] = color_map[class_id]\n",
    "\n",
    "# Create a PIL image from the numpy array\n",
    "img = Image.fromarray(rgb_image)\n",
    "\n",
    "# Save the image to a file\n",
    "img.save(\"rasterized_prediction.png\")\n",
    "\n",
    "print(\"Rasterized image saved to rasterized_prediction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterized image saved to rasterized_prediction.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Assuming y_test is a numpy array or torch tensor containing the\n",
    "# class labels for the entire 1024x1024 image.\n",
    "# Example:\n",
    "# y_test = ... # Your test labels (numpy array or torch tensor)\n",
    "\n",
    "# If y_test is a torch tensor, convert it to a numpy array\n",
    "if isinstance(raw_classes, torch.Tensor):\n",
    "    raw_classes = raw_classes.cpu().numpy()  # Move to CPU if it's on GPU\n",
    "\n",
    "# Ensure y_test is of integer type\n",
    "raw_classes = raw_classes.astype(np.uint8)\n",
    "\n",
    "# Create a color mapping for the classes\n",
    "color_map = {\n",
    "    0: [0, 0, 0],      # background: black\n",
    "    1: [255, 0, 0],    # plantation: red\n",
    "    2: [0, 255, 0],    # grassland_shrubland: green\n",
    "    3: [0, 0, 255],    # mining: blue\n",
    "    4: [255, 255, 0]     # logging: yellow\n",
    "}\n",
    "\n",
    "# Create an RGB image where each class is represented by its color\n",
    "height, width = raw_classes.shape\n",
    "rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        class_id = raw_classes[i, j]\n",
    "        rgb_image[i, j] = color_map[class_id]\n",
    "\n",
    "# Create a PIL image from the numpy array\n",
    "img = Image.fromarray(rgb_image)\n",
    "\n",
    "# Save the image to a file\n",
    "img.save(\"rasterized_prediction.png\")\n",
    "\n",
    "print(\"Rasterized image saved to rasterized_prediction.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
