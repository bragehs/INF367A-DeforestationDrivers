{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/satlaspretrain/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import PreProcessing\n",
    "import satlaspretrain_models\n",
    "from training import *\n",
    "from postprocessing import PostProcessing\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_manager = satlaspretrain_models.Weights()\n",
    "satlas_model = weights_manager.get_pretrained_model(\"Sentinel2_SwinT_SI_MS\", fpn=True, \n",
    "                                             head=satlaspretrain_models.Head.SEGMENT, \n",
    "                                                num_categories=5, device= \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "satlas_model = satlas_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "own_model = EnhancedSatelliteSegmentationModel(in_channels=12, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, save_path, model_name, satlas = True):\n",
    "    # 1. Initialize model architecture\n",
    "    \n",
    "    # 2. Load the saved weights\n",
    "    weights_path = save_path + model_name\n",
    "    state = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "    if satlas:\n",
    "        model.load_state_dict(state['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(state)\n",
    "    # 3. Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(own_model, '/Users/bragehs/Documents/weights/', \"logdiceloss.pth\", satlas=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSatelliteSegmentationModel(\n",
      "  (conv1): Conv2d(12, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (enc1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (enc2): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc3): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc4): ResidualBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (aspp): ASPP(\n",
      "    (aspp1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (aspp2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
      "    (aspp3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "    (aspp4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (conv_out): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (dec1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (final_up1): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_up2): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_conv): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing evaluation_0.tif\n",
      "Processing evaluation_1.tif\n",
      "Processing evaluation_2.tif\n",
      "Processing evaluation_3.tif\n",
      "Processing evaluation_4.tif\n",
      "Processing evaluation_5.tif\n",
      "Processing evaluation_6.tif\n",
      "Processing evaluation_7.tif\n",
      "Processing evaluation_8.tif\n",
      "Processing evaluation_9.tif\n",
      "Processing evaluation_10.tif\n",
      "Processing evaluation_11.tif\n",
      "Processing evaluation_12.tif\n",
      "Processing evaluation_13.tif\n",
      "Processing evaluation_14.tif\n",
      "Processing evaluation_15.tif\n",
      "Processing evaluation_16.tif\n",
      "Processing evaluation_17.tif\n",
      "Processing evaluation_18.tif\n",
      "Processing evaluation_19.tif\n",
      "Processing evaluation_20.tif\n",
      "Processing evaluation_21.tif\n",
      "Processing evaluation_22.tif\n",
      "Processing evaluation_23.tif\n",
      "Processing evaluation_24.tif\n",
      "Processing evaluation_25.tif\n",
      "Processing evaluation_26.tif\n",
      "Processing evaluation_27.tif\n",
      "Processing evaluation_28.tif\n",
      "Processing evaluation_29.tif\n",
      "Processing evaluation_30.tif\n",
      "Processing evaluation_31.tif\n",
      "Processing evaluation_32.tif\n",
      "Processing evaluation_33.tif\n",
      "Processing evaluation_34.tif\n",
      "Processing evaluation_35.tif\n",
      "Processing evaluation_36.tif\n",
      "Processing evaluation_37.tif\n",
      "Processing evaluation_38.tif\n",
      "Processing evaluation_39.tif\n",
      "Processing evaluation_40.tif\n",
      "Processing evaluation_41.tif\n",
      "Processing evaluation_42.tif\n",
      "Processing evaluation_43.tif\n",
      "Processing evaluation_44.tif\n",
      "Processing evaluation_45.tif\n",
      "Processing evaluation_46.tif\n",
      "Processing evaluation_47.tif\n",
      "Processing evaluation_48.tif\n",
      "Processing evaluation_49.tif\n",
      "Processing evaluation_50.tif\n",
      "Processing evaluation_51.tif\n",
      "Processing evaluation_52.tif\n",
      "Processing evaluation_53.tif\n",
      "Processing evaluation_54.tif\n",
      "Processing evaluation_55.tif\n",
      "Processing evaluation_56.tif\n",
      "Processing evaluation_57.tif\n",
      "Processing evaluation_58.tif\n",
      "Processing evaluation_59.tif\n",
      "Processing evaluation_60.tif\n",
      "Processing evaluation_61.tif\n",
      "Processing evaluation_62.tif\n",
      "Processing evaluation_63.tif\n",
      "Processing evaluation_64.tif\n",
      "Processing evaluation_65.tif\n",
      "Processing evaluation_66.tif\n",
      "Processing evaluation_67.tif\n",
      "Processing evaluation_68.tif\n",
      "Processing evaluation_69.tif\n",
      "Processing evaluation_70.tif\n",
      "Processing evaluation_71.tif\n",
      "Processing evaluation_72.tif\n",
      "Processing evaluation_73.tif\n",
      "Processing evaluation_74.tif\n",
      "Processing evaluation_75.tif\n",
      "Processing evaluation_76.tif\n",
      "Processing evaluation_77.tif\n",
      "Processing evaluation_78.tif\n",
      "Processing evaluation_79.tif\n",
      "Processing evaluation_80.tif\n",
      "Processing evaluation_81.tif\n",
      "Processing evaluation_82.tif\n",
      "Processing evaluation_83.tif\n",
      "Processing evaluation_84.tif\n",
      "Processing evaluation_85.tif\n",
      "Processing evaluation_86.tif\n",
      "Processing evaluation_87.tif\n",
      "Processing evaluation_88.tif\n",
      "Processing evaluation_89.tif\n",
      "Processing evaluation_90.tif\n",
      "Processing evaluation_91.tif\n",
      "Processing evaluation_92.tif\n",
      "Processing evaluation_93.tif\n",
      "Processing evaluation_94.tif\n",
      "Processing evaluation_95.tif\n",
      "Processing evaluation_96.tif\n",
      "Processing evaluation_97.tif\n",
      "Processing evaluation_98.tif\n",
      "Processing evaluation_99.tif\n",
      "Processing evaluation_100.tif\n",
      "Processing evaluation_101.tif\n",
      "Processing evaluation_102.tif\n",
      "Processing evaluation_103.tif\n",
      "Processing evaluation_104.tif\n",
      "Processing evaluation_105.tif\n",
      "Processing evaluation_106.tif\n",
      "Processing evaluation_107.tif\n",
      "Processing evaluation_108.tif\n",
      "Processing evaluation_109.tif\n",
      "Processing evaluation_110.tif\n",
      "Processing evaluation_111.tif\n",
      "Processing evaluation_112.tif\n",
      "Processing evaluation_113.tif\n",
      "Processing evaluation_114.tif\n",
      "Processing evaluation_115.tif\n",
      "Processing evaluation_116.tif\n",
      "Processing evaluation_117.tif\n",
      "self.prepared_data.shape: (118, 12, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_data = PreProcessing(train_set=False, satlas=False)\n",
    "test_data.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 12, 1024, 1024)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.prepared_data\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = PostProcessing(loaded_model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m polygons \u001b[38;5;241m=\u001b[39m \u001b[43mpost\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/INF367A-DeforestationDrivers/postprocessing.py:216\u001b[0m, in \u001b[0;36mPostProcessing.post_process\u001b[0;34m(self, gamma)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost_process\u001b[39m(\u001b[38;5;28mself\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m):\n\u001b[0;32m--> 216\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     reconstructed_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstruct_from_patches(predictions)\n\u001b[1;32m    218\u001b[0m     assigned_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_process_torch(reconstructed_images, gamma\u001b[38;5;241m=\u001b[39mgamma)\n",
      "File \u001b[0;32m~/Documents/INF367A-DeforestationDrivers/postprocessing.py:102\u001b[0m, in \u001b[0;36mPostProcessing.predict_probs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m x_coords[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    101\u001b[0m     y \u001b[38;5;241m=\u001b[39m y_coords[i]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m--> 102\u001b[0m     pixel_probs \u001b[38;5;241m=\u001b[39m \u001b[43mprobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Keep as tensor\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     image_predictions\u001b[38;5;241m.\u001b[39mappend((x, y, pixel_probs))\n\u001b[1;32m    104\u001b[0m batch_predictions\u001b[38;5;241m.\u001b[39mextend(image_predictions)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "polygons = post.post_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset class for test data without labels\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,  # Shape: (N, C, H, W)\n",
    "        patch_size: int = 256,\n",
    "        stride: int = 256\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.patches, self.positions = self._create_patches()\n",
    "\n",
    "    def _create_patches(self):\n",
    "        \"\"\"Create patches and store their original positions\"\"\"\n",
    "        patches = []\n",
    "        positions = []  # Store (image_idx, y, x) for each patch\n",
    "        N, C, H, W = self.images.shape\n",
    "\n",
    "        for img_idx in range(N):\n",
    "            image = self.images[img_idx]\n",
    "            for y in range(0, H - self.patch_size + 1, self.stride):\n",
    "                for x in range(0, W - self.patch_size + 1, self.stride):\n",
    "                    img_patch = image[:, y:y + self.patch_size, x:x + self.patch_size]\n",
    "                    patches.append(img_patch)\n",
    "                    positions.append((img_idx, y, x))\n",
    "\n",
    "        return patches, positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.as_tensor(self.patches[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    images=test_data[:2],\n",
    "    patch_size=128,\n",
    "    stride=128\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "                            test_dataset, \n",
    "                            batch_size=64, \n",
    "                            shuffle=False, \n",
    "                            num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)[0]  # Shape: (batch_size, 5, 128, 128)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            # Permute to (batch_size, 128, 128, 5) and collect\n",
    "            probs = probs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    # Concatenate all batches into (num_patches, 128, 128, 5)\n",
    "    return np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_probs(loaded_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_patches(prob_patches, positions, image_shape=(1024, 1024), patch_size=128):\n",
    "    num_classes = prob_patches.shape[-1]\n",
    "    stitched_images = []\n",
    "    # Extract unique image indices from positions\n",
    "    image_indices = sorted(set(pos[0] for pos in positions))\n",
    "    for img_idx in image_indices:\n",
    "        full_image = np.zeros((image_shape[0], image_shape[1], num_classes))\n",
    "        # Iterate through all patches and place them if they belong to the current image\n",
    "        for i, (curr_idx, y, x) in enumerate(positions):\n",
    "            if curr_idx == img_idx:\n",
    "                full_image[y:y+patch_size, x:x+patch_size, :] = prob_patches[i]\n",
    "        stitched_images.append(full_image)\n",
    "    return [torch.tensor(img) for img in stitched_images]\n",
    "\n",
    "\n",
    "def post_process_torch(outputs, gamma=0.5):\n",
    "    all_images = []\n",
    "    for img in outputs:  # Each img is a tensor of shape (1024, 1024, 5)\n",
    "        # Pad the image to handle borders\n",
    "        padded = torch.nn.functional.pad(img, (0, 0, 1, 1, 1, 1))  # Pad H and W by 1\n",
    "        \n",
    "        # Compute contributions from 4-directional neighbors\n",
    "        top = padded[:-2, 1:-1, :]    # Shape: (1024, 1024, 5)\n",
    "        bottom = padded[2:, 1:-1, :]\n",
    "        left = padded[1:-1, :-2, :]\n",
    "        right = padded[1:-1, 2:, :]\n",
    "        \n",
    "        # Combine neighbor contributions and add to original probabilities\n",
    "        neighbors = gamma * (top + bottom + left + right)\n",
    "        combined_probs = img + neighbors\n",
    "        \n",
    "        # Get final predictions\n",
    "        class_predictions = torch.argmax(combined_probs, dim=2)\n",
    "        all_images.append(class_predictions)\n",
    "    \n",
    "    return all_images  # List of (1024, 1024) tensors with class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = test_dataloader.dataset.positions  # Get positions from dataset\n",
    "stitched_tensors = stitch_patches(preds, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2092, 0.1700, 0.2911, 0.1693, 0.1603], dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_tensors[1][0][1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed = post_process_torch(stitched_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(tensors):\n",
    "    \"\"\"\n",
    "    Convert multiple image tensors to polygons.\n",
    "    \n",
    "    Args:\n",
    "        tensors: List of tensors or single tensor. Each tensor should be (1024, 1024) \n",
    "                containing class labels 0-4 (either numpy array or torch.Tensor)\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries, one per image, where each dictionary\n",
    "                contains polygons for each class {0: [...], 1: [...], ...}\n",
    "    \"\"\"\n",
    "    # Handle single tensor case and convert to numpy\n",
    "    if isinstance(tensors, (np.ndarray, torch.Tensor)):\n",
    "        tensors = [tensors]\n",
    "    \n",
    "    all_image_polygons = []\n",
    "    \n",
    "    for idx, tensor in enumerate(tensors):\n",
    "        print(\"Converting tensor to polygons for image\", idx)\n",
    "        image_polygons = {}\n",
    "        \n",
    "        # Convert to numpy and ensure proper 2D format\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            # Handle both CPU and GPU tensors\n",
    "            tensor_np = tensor.cpu().detach().numpy().squeeze().astype(np.uint8)\n",
    "            print(tensor_np.shape)\n",
    "        else:\n",
    "            tensor_np = tensor.squeeze().astype(np.uint8)\n",
    "        \n",
    "        # Critical validation\n",
    "        if tensor_np.ndim != 2:\n",
    "            raise ValueError(f\"Input tensor must be 2D after squeezing. Got {tensor_np.shape}\")\n",
    "        \n",
    "        for class_id in range(5):\n",
    "            # Create binary mask\n",
    "            mask = (tensor_np == class_id).astype(np.uint8)\n",
    "            \n",
    "            # Verify OpenCV requirements\n",
    "            if not isinstance(mask, np.ndarray):\n",
    "                raise TypeError(f\"Mask must be numpy array, got {type(mask)}\")\n",
    "            if mask.dtype != np.uint8:\n",
    "                mask = mask.astype(np.uint8)\n",
    "            \n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(\n",
    "                mask, \n",
    "                cv2.RETR_EXTERNAL, \n",
    "                cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            \n",
    "            # Convert and filter contours\n",
    "            image_polygons[class_id] = [\n",
    "                contour.squeeze().tolist() \n",
    "                for contour in contours \n",
    "                if contour.shape[0] >= 3  # Minimum 3 points for polygon\n",
    "            ]\n",
    "        \n",
    "        all_image_polygons.append(image_polygons)\n",
    "    \n",
    "    return all_image_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tensor to polygons for image 0\n",
      "(1024, 1024)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mask must be numpy array, got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m polygons \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_processed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[190], line 41\u001b[0m, in \u001b[0;36mconverter\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Verify OpenCV requirements\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask must be numpy array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mask)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m     43\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mTypeError\u001b[0m: Mask must be numpy array, got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "polygons = converter(post_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satlaspretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
