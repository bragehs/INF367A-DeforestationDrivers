{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <FB2FD416-6C4D-3621-B677-61F07C02A3C5> /Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/martin/anaconda3/envs/geospatial/lib/python3.9/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/martin/anaconda3/envs/geospatial/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "import satlaspretrain_models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from dataLoaderAugment import SatelliteSegmentationDataset\n",
    "from preprocessing import ProcessData\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, multiclass=False):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.multiclass = multiclass\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        pred = torch.softmax(pred, dim=1)\n",
    "        \n",
    "        if self.multiclass:\n",
    "            target = F.one_hot(target, num_classes=pred.shape[1]).permute(0, 3, 1, 2).float()\n",
    "            \n",
    "        numerator = 2 * (pred * target).sum(dim=(2, 3))\n",
    "        denominator = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        \n",
    "        dice_score = 1 - (numerator + 1) / (denominator + 1)\n",
    "        return dice_score.mean()\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        ce_loss = F.cross_entropy(pred, target, weight=self.alpha, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=None):\n",
    "        super().__init__()\n",
    "        self.focal = FocalLoss(alpha=alpha, gamma=2.0)\n",
    "        self.dice = DiceLoss(multiclass=True)\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        focal_loss = self.focal(pred, target)\n",
    "        dice_loss = self.dice(pred, target)\n",
    "        return 0.5 * focal_loss + 0.5 * dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(\n",
    "    train_images: np.ndarray,\n",
    "    train_labels: np.ndarray,\n",
    "    val_images: np.ndarray,\n",
    "    val_labels: np.ndarray,\n",
    "    batch_size: int = 8,\n",
    "    patch_size: int = 256,\n",
    "    patch_stride: int = 128,\n",
    "    num_workers: int = 4\n",
    ") -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders with the enhanced dataset.\n",
    "    \"\"\"\n",
    "    train_dataset = SatelliteSegmentationDataset(\n",
    "        images=train_images,\n",
    "        labels=train_labels,\n",
    "        patch_size=patch_size,\n",
    "        patch_stride=patch_stride,\n",
    "        augment=True,\n",
    "        max_patches_per_image=32\n",
    "    )\n",
    "    \n",
    "    val_dataset = SatelliteSegmentationDataset(\n",
    "        images=val_images,\n",
    "        labels=val_labels,\n",
    "        patch_size=patch_size,\n",
    "        patch_stride=patch_size,\n",
    "        augment=False\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    val_images,\n",
    "    val_labels,\n",
    "    num_epochs=20,\n",
    "    batch_size=4,\n",
    "    patch_size=128,\n",
    "    patch_stride=64,\n",
    "    learning_rate=1e-4,\n",
    "    device='cpu',\n",
    "    save_dir='../models'\n",
    "):\n",
    "    \"\"\"\n",
    "    Complete training function with optimized components for satellite image segmentation.\n",
    "    \n",
    "    Args:\n",
    "        model: The pre-trained model to fine-tune\n",
    "        train_images: Training images array (N, C, H, W)\n",
    "        train_labels: Training labels array (N, H, W)\n",
    "        val_images: Validation images array\n",
    "        val_labels: Validation labels array\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        patch_size: Size of image patches\n",
    "        patch_stride: Stride for patch extraction\n",
    "        learning_rate: Initial learning rate\n",
    "        device: Device to train on ('cuda' or 'cpu')\n",
    "        save_dir: Directory to save model checkpoints\n",
    "    \"\"\"\n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        train_images=train_images,\n",
    "        train_labels=train_labels,\n",
    "        val_images=val_images,\n",
    "        val_labels=val_labels,\n",
    "        batch_size=batch_size,\n",
    "        patch_size=patch_size,\n",
    "        patch_stride=patch_stride\n",
    "    )\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        weight_decay=0.1\n",
    "    )\n",
    "    \n",
    "    # Initialize learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=len(train_loader),\n",
    "        pct_start=0.2,  # 10% warmup\n",
    "        div_factor=25,  # initial_lr = max_lr/10\n",
    "        final_div_factor=1000  # final_lr = initial_lr/100\n",
    "    )\n",
    "    \n",
    "    # Initialize loss function\n",
    "    class_weights = torch.tensor([0.1, 1.0, 1.0, 1.0, 1.0]).to(device)\n",
    "    criterion = CombinedLoss(alpha=class_weights)\n",
    "    \n",
    "    # Setup tensorboard and checkpoints\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    writer = SummaryWriter(f'runs/experiment_{timestamp}')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(progress_bar):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output[0], target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            # Log learning rate\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            writer.add_scalar('Training/LR', current_lr, \n",
    "                            epoch * len(train_loader) + batch_idx)\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'batch_loss': f'{loss.item():.4f}',\n",
    "                'avg_loss': f'{epoch_loss/num_batches:.4f}',\n",
    "                'lr': f'{current_lr:.6f}'\n",
    "            })\n",
    "            \n",
    "            # Log batch-level metrics\n",
    "            writer.add_scalar('Training/BatchLoss', loss.item(), \n",
    "                            epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "        # Log epoch-level metrics\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        writer.add_scalar('Training/EpochLoss', avg_epoch_loss, epoch)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for val_data, val_target in tqdm.tqdm(val_loader, desc='Validation'):\n",
    "                val_data, val_target = val_data.to(device), val_target.to(device)\n",
    "                \n",
    "                val_output = model(val_data)\n",
    "                batch_loss = criterion(val_output[0], val_target)\n",
    "                val_loss += batch_loss.item()\n",
    "                \n",
    "                # Collect predictions and targets\n",
    "                pred = val_output[0].argmax(dim=1).cpu().numpy()\n",
    "                val_predictions.extend(pred.flatten())\n",
    "                val_targets.extend(val_target.cpu().numpy().flatten())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_f1 = f1_score(val_targets, val_predictions, average='weighted')\n",
    "        \n",
    "        # Log validation metrics\n",
    "        writer.add_scalar('Validation/Loss', avg_val_loss, epoch)\n",
    "        writer.add_scalar('Validation/F1', val_f1, epoch)\n",
    "        \n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Training Loss: {avg_epoch_loss:.4f}')\n",
    "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
    "        print(f'Validation F1: {val_f1:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'training_loss': avg_epoch_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f'{save_dir}/best_model_{timestamp}.pth')\n",
    "        \n",
    "        # Save regular checkpoint every 5 epochs\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'val_f1': val_f1,\n",
    "                'training_loss': avg_epoch_loss,\n",
    "                'val_loss': avg_val_loss,\n",
    "            }, f'{save_dir}/checkpoint_epoch_{epoch+1}_{timestamp}.pth')\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from /Users/martin/Desktop/inf367project/INF367A-DeforestationDrivers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/martin/anaconda3/envs/geospatial/lib/python3.9/site-packages/satlaspretrain_models/model.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weights = torch.load(weights_file, map_location=torch.device('cpu'))\n",
      "Epoch 1/20: 100%|██████████| 80/80 [02:16<00:00,  1.71s/it, batch_loss=0.6870, avg_loss=0.6625, lr=0.000018]\n",
      "Validation: 100%|██████████| 79/79 [01:07<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20:\n",
      "Training Loss: 0.6625\n",
      "Validation Loss: 0.6949\n",
      "Validation F1: 0.3094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 80/80 [02:06<00:00,  1.58s/it, batch_loss=0.6749, avg_loss=0.6225, lr=0.000052]\n",
      "Validation: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/20:\n",
      "Training Loss: 0.6225\n",
      "Validation Loss: 0.6808\n",
      "Validation F1: 0.2826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 80/80 [02:26<00:00,  1.83s/it, batch_loss=0.6414, avg_loss=0.6256, lr=0.000086]\n",
      "Validation: 100%|██████████| 79/79 [01:09<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/20:\n",
      "Training Loss: 0.6256\n",
      "Validation Loss: 0.6913\n",
      "Validation F1: 0.2819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 80/80 [02:06<00:00,  1.58s/it, batch_loss=0.9397, avg_loss=0.6147, lr=0.000100]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/20:\n",
      "Training Loss: 0.6147\n",
      "Validation Loss: 0.8104\n",
      "Validation F1: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 80/80 [02:15<00:00,  1.69s/it, batch_loss=0.7428, avg_loss=0.6322, lr=0.000099]\n",
      "Validation: 100%|██████████| 79/79 [01:07<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/20:\n",
      "Training Loss: 0.6322\n",
      "Validation Loss: 0.8104\n",
      "Validation F1: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 80/80 [02:22<00:00,  1.78s/it, batch_loss=0.6478, avg_loss=0.6369, lr=0.000096]\n",
      "Validation: 100%|██████████| 79/79 [01:10<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/20:\n",
      "Training Loss: 0.6369\n",
      "Validation Loss: 0.8104\n",
      "Validation F1: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 80/80 [02:24<00:00,  1.80s/it, batch_loss=0.6707, avg_loss=0.6231, lr=0.000092]\n",
      "Validation: 100%|██████████| 79/79 [01:11<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/20:\n",
      "Training Loss: 0.6231\n",
      "Validation Loss: 0.8104\n",
      "Validation F1: 0.0788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 80/80 [02:28<00:00,  1.86s/it, batch_loss=0.6764, avg_loss=0.6360, lr=0.000085]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/20:\n",
      "Training Loss: 0.6360\n",
      "Validation Loss: 0.6821\n",
      "Validation F1: 0.2858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 80/80 [02:33<00:00,  1.92s/it, batch_loss=0.7649, avg_loss=0.6113, lr=0.000078]\n",
      "Validation: 100%|██████████| 79/79 [01:09<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/20:\n",
      "Training Loss: 0.6113\n",
      "Validation Loss: 0.7155\n",
      "Validation F1: 0.0990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 80/80 [02:28<00:00,  1.85s/it, batch_loss=0.6994, avg_loss=0.6096, lr=0.000069]\n",
      "Validation: 100%|██████████| 79/79 [01:10<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/20:\n",
      "Training Loss: 0.6096\n",
      "Validation Loss: 0.6972\n",
      "Validation F1: 0.3450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 80/80 [02:34<00:00,  1.93s/it, batch_loss=0.6845, avg_loss=0.5988, lr=0.000060]\n",
      "Validation: 100%|██████████| 79/79 [01:10<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/20:\n",
      "Training Loss: 0.5988\n",
      "Validation Loss: 0.7178\n",
      "Validation F1: 0.2991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 80/80 [02:36<00:00,  1.96s/it, batch_loss=0.6459, avg_loss=0.5816, lr=0.000050]\n",
      "Validation: 100%|██████████| 79/79 [01:21<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12/20:\n",
      "Training Loss: 0.5816\n",
      "Validation Loss: 0.6899\n",
      "Validation F1: 0.3465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 80/80 [03:08<00:00,  2.36s/it, batch_loss=0.5820, avg_loss=0.5717, lr=0.000040]\n",
      "Validation: 100%|██████████| 79/79 [01:07<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13/20:\n",
      "Training Loss: 0.5717\n",
      "Validation Loss: 0.7088\n",
      "Validation F1: 0.3210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 80/80 [02:29<00:00,  1.86s/it, batch_loss=0.5560, avg_loss=0.5598, lr=0.000031]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14/20:\n",
      "Training Loss: 0.5598\n",
      "Validation Loss: 0.6928\n",
      "Validation F1: 0.3497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 80/80 [02:19<00:00,  1.74s/it, batch_loss=0.6246, avg_loss=0.5565, lr=0.000022]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15/20:\n",
      "Training Loss: 0.5565\n",
      "Validation Loss: 0.7249\n",
      "Validation F1: 0.2938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 80/80 [02:38<00:00,  1.98s/it, batch_loss=0.5972, avg_loss=0.5551, lr=0.000015]\n",
      "Validation: 100%|██████████| 79/79 [01:12<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16/20:\n",
      "Training Loss: 0.5551\n",
      "Validation Loss: 0.6534\n",
      "Validation F1: 0.4022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 80/80 [02:27<00:00,  1.84s/it, batch_loss=0.6514, avg_loss=0.5460, lr=0.000008]\n",
      "Validation: 100%|██████████| 79/79 [01:07<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17/20:\n",
      "Training Loss: 0.5460\n",
      "Validation Loss: 0.6635\n",
      "Validation F1: 0.3903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 80/80 [02:31<00:00,  1.90s/it, batch_loss=0.5600, avg_loss=0.5490, lr=0.000004]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18/20:\n",
      "Training Loss: 0.5490\n",
      "Validation Loss: 0.6566\n",
      "Validation F1: 0.3998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 80/80 [02:35<00:00,  1.94s/it, batch_loss=0.5645, avg_loss=0.5487, lr=0.000001]\n",
      "Validation: 100%|██████████| 79/79 [01:08<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19/20:\n",
      "Training Loss: 0.5487\n",
      "Validation Loss: 0.6634\n",
      "Validation F1: 0.3914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 80/80 [02:35<00:00,  1.95s/it, batch_loss=0.5760, avg_loss=0.5475, lr=0.000000]\n",
      "Validation: 100%|██████████| 79/79 [01:09<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/20:\n",
      "Training Loss: 0.5475\n",
      "Validation Loss: 0.6607\n",
      "Validation F1: 0.3949\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # Initialize ProcessData and model\n",
    "    data = ProcessData()\n",
    "    data.prepared_data, data.labels = data.load_preprocessed_data()\n",
    "    data.prepared_data = data.prepared_data[:, :9, :, :]\n",
    "    \n",
    "    # Create train/val split\n",
    "    train_sample = 20\n",
    "    val_sample = 15\n",
    "    \n",
    "    # Initialize model\n",
    "    weights_manager = satlaspretrain_models.Weights()\n",
    "    model = weights_manager.get_pretrained_model(\n",
    "        \"Sentinel2_SwinT_SI_MS\",\n",
    "        fpn=True,\n",
    "        head=satlaspretrain_models.Head.SEGMENT,\n",
    "        num_categories=5,\n",
    "        device='cpu'\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    trained_model = train_model(\n",
    "        model=model,\n",
    "        train_images=data.prepared_data[:train_sample],\n",
    "        train_labels=data.labels[:train_sample],\n",
    "        val_images=data.prepared_data[train_sample:train_sample + val_sample],\n",
    "        val_labels=data.labels[train_sample:train_sample + val_sample],\n",
    "        num_epochs=20,\n",
    "        batch_size=8,\n",
    "        device=device\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geospatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
