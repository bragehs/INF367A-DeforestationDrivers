{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import ProcessData\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import timm\n",
    "import satlaspretrain_models\n",
    "from models import UNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'weights_only'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[217], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m weights_manager \u001b[38;5;241m=\u001b[39m \u001b[43msatlaspretrain_models\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWeights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m weights_manager\u001b[38;5;241m.\u001b[39mget_pretrained_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentinel2_Resnet50_SI_MS\u001b[39m\u001b[38;5;124m\"\u001b[39m, fpn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m      3\u001b[0m                                              head\u001b[38;5;241m=\u001b[39msatlaspretrain_models\u001b[38;5;241m.\u001b[39mHead\u001b[38;5;241m.\u001b[39mSEGMENT, \n\u001b[1;32m      4\u001b[0m                                                 num_categories\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, device\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'weights_only'"
     ]
    }
   ],
   "source": [
    "weights_manager = satlaspretrain_models.Weights()\n",
    "model = weights_manager.get_pretrained_model(\"Sentinel2_Resnet50_SI_MS\", fpn=True, \n",
    "                                             head=satlaspretrain_models.Head.SEGMENT, \n",
    "                                                num_categories=5, device= \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define patch size and stride\n",
    "PATCH_SIZE = 128  # Adjust as needed (256, 512, etc.)\n",
    "STRIDE = 128  # Overlapping patches\n",
    "\n",
    "class Sentinel2SegmentationDataset(Dataset):\n",
    "    def __init__(self, images, labels, patch_size=PATCH_SIZE, stride=STRIDE, transform=None):\n",
    "        \"\"\"\n",
    "        images: Tensor or numpy array of shape (N, 12, H, W)\n",
    "        labels: Tensor or numpy array of shape (N, H, W)\n",
    "        patch_size: Size of the patches (default 256)\n",
    "        stride: Stride for patching (default 128 for overlapping)\n",
    "        transform: Optional image transformations\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.transform = transform\n",
    "        self.patches = []  # Store (image_patch, label_patch) pairs\n",
    "\n",
    "        self.create_patches()\n",
    "\n",
    "    def create_patches(self):\n",
    "        \"\"\"Extracts patches from the dataset.\"\"\"\n",
    "        N, C, H, W = self.images.shape\n",
    "\n",
    "        for i in range(N):\n",
    "            img = self.images[i]  # Shape (12, H, W)\n",
    "            lbl = self.labels[i]  # Shape (H, W)\n",
    "\n",
    "            # Divide the 1024 image dimension into 512x512 patches with no overlap\n",
    "            for y in range(0, H, self.patch_size):\n",
    "                for x in range(0, W, self.patch_size):\n",
    "                    img_patch = img[:, y:y+self.patch_size, x:x+self.patch_size]\n",
    "                    lbl_patch = lbl[y:y+self.patch_size, x:x+self.patch_size]\n",
    "                    self.patches.append((img_patch, lbl_patch))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_patch, lbl_patch = self.patches[idx]\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            img_patch = self.transform(img_patch)\n",
    "\n",
    "        return torch.as_tensor(img_patch, dtype=torch.float32), torch.as_tensor(lbl_patch, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = ProcessData()\n",
    "#data.preprocess()\n",
    "#data.save_preprocessed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded preprocessed data from /Users/bragehs/Documents/INF367A-DeforestationDrivers\n"
     ]
    }
   ],
   "source": [
    "data = ProcessData()\n",
    "data.prepared_data, data.labels = data.load_preprocessed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # Uses the first available GPU\n",
    "    print(\"Using:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bands = [1, 2, 3, 4, 5, 6, 7, 10, 11]\n",
    "#prepared_data = data.prepared_data[:, bands]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = data.prepared_data.astype('float32') / 10000\n",
    "#prepared_data = prepared_data.astype('float32') / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((176, 12, 1024, 1024), (176, 1024, 1024))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First split into train and temp (val + test)\n",
    "X_train, X_val, y_train, y_val = train_test_split(prepared_data, labels, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bragehs/Documents/INF367A-DeforestationDrivers/models.py:211: UserWarning: nn.init.xavier_normal is now deprecated in favor of nn.init.xavier_normal_.\n",
      "  init.xavier_normal(m.weight)\n",
      "/Users/bragehs/Documents/INF367A-DeforestationDrivers/models.py:212: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "UNet_model = UNet(in_channels=12, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSatelliteSegmentationModel(\n",
      "  (conv1): Conv2d(12, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (enc1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (enc2): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc3): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc4): ResidualBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (aspp): ASPP(\n",
      "    (aspp1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (aspp2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
      "    (aspp3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "    (aspp4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (conv_out): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (dec1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (final_up1): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_up2): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_conv): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "class ASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ASPP, self).__init__()\n",
    "        self.aspp1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        self.aspp2 = nn.Conv2d(in_channels, out_channels, 3, padding=6, dilation=6, bias=False)\n",
    "        self.aspp3 = nn.Conv2d(in_channels, out_channels, 3, padding=12, dilation=12, bias=False)\n",
    "        self.aspp4 = nn.Conv2d(in_channels, out_channels, 3, padding=18, dilation=18, bias=False)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n",
    "        \n",
    "        self.conv_out = nn.Conv2d(out_channels * 5, out_channels, 1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        x1 = self.aspp1(x)\n",
    "        x2 = self.aspp2(x)\n",
    "        x3 = self.aspp3(x)\n",
    "        x4 = self.aspp4(x)\n",
    "        x5 = self.avg_pool(x)\n",
    "        x5 = self.conv1(x5)\n",
    "        x5 = F.interpolate(x5, size=size, mode='bilinear', align_corners=True)\n",
    "        x = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "        x = self.conv_out(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class EnhancedSatelliteSegmentationModel(nn.Module):\n",
    "    def __init__(self, in_channels=12, num_classes=5):\n",
    "        super(EnhancedSatelliteSegmentationModel, self).__init__()\n",
    "        \n",
    "        # Initial Conv Block\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Encoder path with residual blocks\n",
    "        self.enc1 = ResidualBlock(64, 64)\n",
    "        self.enc2 = ResidualBlock(64, 128, stride=2)\n",
    "        self.enc3 = ResidualBlock(128, 256, stride=2)\n",
    "        self.enc4 = ResidualBlock(256, 512, stride=2)\n",
    "        \n",
    "        # ASPP module\n",
    "        self.aspp = ASPP(512, 256)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.skip1 = nn.Conv2d(256, 256, kernel_size=1)\n",
    "        \n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.skip2 = nn.Conv2d(128, 128, kernel_size=1)\n",
    "        \n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.skip3 = nn.Conv2d(64, 64, kernel_size=1)\n",
    "        \n",
    "        # Additional upsampling to match input resolution\n",
    "        self.final_up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final_up2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.final_conv = nn.Sequential(\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, num_classes, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Store input size for potential interpolation\n",
    "        input_size = x.size()[2:]\n",
    "        \n",
    "        # Initial convolution\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # Encoder path with skip connections\n",
    "        skip1 = self.enc1(x)\n",
    "        skip2 = self.enc2(skip1)\n",
    "        skip3 = self.enc3(skip2)\n",
    "        x = self.enc4(skip3)\n",
    "        \n",
    "        # ASPP module\n",
    "        x = self.aspp(x)\n",
    "        \n",
    "        # Decoder path with skip connections\n",
    "        x = self.dec1(x)\n",
    "        x = x + self.skip1(skip3)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dec2(x)\n",
    "        x = x + self.skip2(skip2)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dec3(x)\n",
    "        x = x + self.skip3(skip1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Additional upsampling to match input resolution\n",
    "        x = self.final_up1(x)\n",
    "        x = self.final_up2(x)\n",
    "        x = self.final_conv(x)\n",
    "        \n",
    "        # Optional: force output to match input spatial dimensions exactly\n",
    "        if x.size()[2:] != input_size:\n",
    "            x = F.interpolate(x, size=input_size, mode='bilinear', align_corners=True)\n",
    "            \n",
    "        return x\n",
    "# Example instantiation (for testing):\n",
    "model = EnhancedSatelliteSegmentationModel(in_channels=12, num_classes=5)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 10,913,861\n"
     ]
    }
   ],
   "source": [
    "model = EnhancedSatelliteSegmentationModel()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(16, 12, 256, 256) \n",
    "output = UNet_model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prepared_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Patch Shape: torch.Size([8, 12, 128, 128])\n",
      "Label Patch Shape: torch.Size([8, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "train_dataset = Sentinel2SegmentationDataset(X_train[:7], y_train[:7])\n",
    "val_dataset = Sentinel2SegmentationDataset(X_val[:7], y_val[:7])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "\n",
    "# Check dataset output\n",
    "for img_patch, lbl_patch in train_dataloader:\n",
    "    print(\"Image Patch Shape:\", img_patch.shape)  # Expected: (batch_size, 9, 512, 512)\n",
    "    print(\"Label Patch Shape:\", lbl_patch.shape)  # Expected: (batch_size, 512, 512)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 448)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__len__(), val_dataset.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 12, 1024, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: [ 0.28784642  1.11019248  1.74212239 28.21244571 63.60237425] for classes: [0 1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "# Convert each label tensor to a NumPy array\n",
    "all_labels = np.concatenate([\n",
    "    y.detach().cpu().numpy().flatten() if torch.is_tensor(y) else np.array(y).flatten()\n",
    "    for _, y in train_dataset\n",
    "])\n",
    "classes = np.unique(all_labels)\n",
    "weights = compute_class_weight('balanced', classes=classes, y=all_labels)\n",
    "print(\"Class weights:\", weights, \"for classes:\", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_weights(model):\n",
    "    # Print first layer weights from a few layers\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name}: mean={param.data.mean():.4f}, std={param.data.std():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight: mean=-0.0000, std=0.0238\n",
      "bn1.weight: mean=1.0000, std=0.0000\n",
      "enc1.conv1.weight: mean=0.0001, std=0.0241\n",
      "enc1.bn1.weight: mean=1.0000, std=0.0000\n",
      "enc1.conv2.weight: mean=-0.0001, std=0.0240\n",
      "enc1.bn2.weight: mean=1.0000, std=0.0000\n",
      "enc2.conv1.weight: mean=0.0000, std=0.0241\n",
      "enc2.bn1.weight: mean=1.0000, std=0.0000\n",
      "enc2.conv2.weight: mean=0.0001, std=0.0170\n",
      "enc2.bn2.weight: mean=1.0000, std=0.0000\n",
      "enc2.shortcut.0.weight: mean=0.0002, std=0.0726\n",
      "enc2.shortcut.1.weight: mean=1.0000, std=0.0000\n",
      "enc3.conv1.weight: mean=-0.0001, std=0.0170\n",
      "enc3.bn1.weight: mean=1.0000, std=0.0000\n",
      "enc3.conv2.weight: mean=-0.0000, std=0.0120\n",
      "enc3.bn2.weight: mean=1.0000, std=0.0000\n",
      "enc3.shortcut.0.weight: mean=0.0001, std=0.0511\n",
      "enc3.shortcut.1.weight: mean=1.0000, std=0.0000\n",
      "enc4.conv1.weight: mean=-0.0000, std=0.0120\n",
      "enc4.bn1.weight: mean=1.0000, std=0.0000\n",
      "enc4.conv2.weight: mean=-0.0000, std=0.0085\n",
      "enc4.bn2.weight: mean=1.0000, std=0.0000\n",
      "enc4.shortcut.0.weight: mean=-0.0001, std=0.0360\n",
      "enc4.shortcut.1.weight: mean=1.0000, std=0.0000\n",
      "aspp.aspp1.weight: mean=0.0001, std=0.0255\n",
      "aspp.aspp2.weight: mean=0.0000, std=0.0085\n",
      "aspp.aspp3.weight: mean=-0.0000, std=0.0085\n",
      "aspp.aspp4.weight: mean=0.0000, std=0.0085\n",
      "aspp.conv1.weight: mean=0.0000, std=0.0255\n",
      "aspp.conv_out.weight: mean=0.0000, std=0.0161\n",
      "aspp.bn.weight: mean=1.0000, std=0.0000\n",
      "dec1.0.weight: mean=-0.0000, std=0.0090\n",
      "dec1.1.weight: mean=1.0000, std=0.0000\n",
      "skip1.weight: mean=0.0002, std=0.0362\n",
      "dec2.0.weight: mean=0.0000, std=0.0128\n",
      "dec2.1.weight: mean=1.0000, std=0.0000\n",
      "skip2.weight: mean=0.0002, std=0.0510\n",
      "dec3.0.weight: mean=-0.0000, std=0.0180\n",
      "dec3.1.weight: mean=1.0000, std=0.0000\n",
      "skip3.weight: mean=-0.0001, std=0.0726\n",
      "final_up1.0.weight: mean=0.0002, std=0.0255\n",
      "final_up1.1.weight: mean=1.0000, std=0.0000\n",
      "final_up2.0.weight: mean=-0.0001, std=0.0256\n",
      "final_up2.1.weight: mean=1.0000, std=0.0000\n",
      "final_conv.0.weight: mean=0.0002, std=0.0338\n",
      "final_conv.1.weight: mean=1.0000, std=0.0000\n",
      "final_conv.3.weight: mean=0.0087, std=0.0954\n"
     ]
    }
   ],
   "source": [
    "print_model_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment arguments.\n",
    "num_epochs = 40\n",
    "val_step = 1  # evaluate every val_step epochs\n",
    "#loss_fn = nn.CrossEntropyLoss(weight=weights)\n",
    "save_path = os.path.split(os.getcwd())[0] + '/weights/'  # where to save model weights\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedSegmentationLoss(nn.Module):\n",
    "    def __init__(self, num_classes, weights=None):\n",
    "        super().__init__()\n",
    "        if weights is not None:\n",
    "            weights = torch.tensor(weights, dtype=torch.float32)\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weights)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        # Ensure pred and target are float32\n",
    "        pred = pred.float()\n",
    "        target = target.long()  # CrossEntropyLoss expects long targets\n",
    "        \n",
    "        # Basic cross entropy\n",
    "        ce_loss = self.ce(pred, target)\n",
    "        \n",
    "        # Add auxiliary losses if needed\n",
    "        aux_loss = 0\n",
    "        \n",
    "        return ce_loss + aux_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogCoshDiceLoss(nn.Module):\n",
    "    def __init__(self, num_classes, epsilon=1e-6):\n",
    "        super(LogCoshDiceLoss, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred shape: (B, C, H, W)\n",
    "        # target shape: (B, H, W)\n",
    "        \n",
    "        # Convert predictions to probabilities\n",
    "        pred_probs = F.softmax(pred, dim=1)  # (B, C, H, W)\n",
    "        \n",
    "        dice_scores = []\n",
    "        for cls in range(self.num_classes):\n",
    "            # Create binary masks for each class\n",
    "            pred_cls = pred_probs[:, cls]  # (B, H, W)\n",
    "            target_cls = (target == cls).float()  # (B, H, W)\n",
    "            \n",
    "            # Calculate intersection and union\n",
    "            intersection = (pred_cls * target_cls).sum(dim=(1, 2))\n",
    "            cardinality = pred_cls.sum(dim=(1, 2)) + target_cls.sum(dim=(1, 2))\n",
    "            \n",
    "            # Calculate Dice coefficient\n",
    "            dice = (2. * intersection + self.epsilon) / (cardinality + self.epsilon)\n",
    "            dice_scores.append(dice)\n",
    "        \n",
    "        # Stack dice scores for all classes\n",
    "        dice_scores = torch.stack(dice_scores, dim=1)  # (B, C)\n",
    "        \n",
    "        # Compute Log-Cosh loss\n",
    "        loss = torch.log(torch.cosh(1. - dice_scores))\n",
    "        \n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedIoULoss(nn.Module):\n",
    "    def __init__(self, weights=None):\n",
    "        super(WeightedIoULoss, self).__init__()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        # pred shape: (B, C, H, W)\n",
    "        # target shape: (B, H, W)\n",
    "        \n",
    "        B, C, H, W = pred.shape\n",
    "        \n",
    "        # Convert predictions to probabilities\n",
    "        pred = F.softmax(pred, dim=1)\n",
    "        \n",
    "        # One-hot encode target\n",
    "        target = target.permute(0, 3, 1, 2).float()\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = (pred * target).sum(dim=(2, 3))  # (B, C)\n",
    "        union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection  # (B, C)\n",
    "        \n",
    "        # Calculate IoU\n",
    "        iou = (intersection + 1e-7) / (union + 1e-7)  # (B, C)\n",
    "        \n",
    "        if self.weights is not None:\n",
    "            # Make sure weights is a tensor on the same device\n",
    "            if not isinstance(self.weights, torch.Tensor):\n",
    "                self.weights = torch.tensor(self.weights, device=pred.device)\n",
    "            iou = iou * self.weights  # Apply class weights\n",
    "        \n",
    "        # Average over classes and batch\n",
    "        iou_loss = 1 - iou.mean()\n",
    "        \n",
    "        return iou_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(val_loader):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "            outputs = model(images)\n",
    "            if isinstance(outputs, tuple):\n",
    "                outputs = outputs[0]  # Get main output if model returns multiple outputs\n",
    "\n",
    "            # Ensure correct shape before loss calculation\n",
    "            if outputs.shape[0] != targets.shape[0]:\n",
    "                outputs = outputs.permute(0, 2, 1)\n",
    "            loss = criterion(outputs, targets)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 256, 256])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_warmup(model, train_loader, val_loader, num_epochs, filename,\n",
    "                      num_warm_up= 5, learning_rate = 1e-3, patience = 5,\n",
    "                      weight_decay = 1e-2):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate, weight_decay = weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=patience)\n",
    "    criterion = LogCoshDiceLoss(num_classes=5)\n",
    "\n",
    "    # Warmup phase\n",
    "    model.train()\n",
    "    model = model.float()\n",
    "    print(\"Starting warmup phase...\")\n",
    "    for epoch in range(num_warm_up):  # epochs of warm up:\n",
    "        with tqdm(train_loader, desc=f'Warmup Epoch {epoch+1}/5') as pbar:\n",
    "            for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                images, targets = images.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                # Ensure correct shape before loss calculation\n",
    "                if outputs.shape[0] != targets.shape[0]:\n",
    "                    outputs.permute(2, 3, 0, 1).contiguous().view(-1, 5)\n",
    "\n",
    "                loss = criterion(outputs, targets) / 4  # Accumulate gradients over 4 steps\n",
    "                loss.backward()\n",
    "\n",
    "                if (batch_idx + 1) % 4 == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                pbar.set_postfix({'loss': loss.item() * 4})  # Show accumulated loss\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Starting main training phase...\")\n",
    "    # Main training phase\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        # Training loop with progress bar\n",
    "        with tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}') as pbar:\n",
    "            for batch_idx, (images, targets) in enumerate(pbar):\n",
    "                images, targets = images.to(device).float(), targets.to(device).float()\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Ensure correct shape before loss calculation\n",
    "                if outputs.shape[0] != targets.shape[0]:\n",
    "                    outputs.permute(2, 3, 0, 1).contiguous().view(-1, 5)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                pbar.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "        # Calculate average training loss\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = validate(model, val_loader, criterion)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path + filename)\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_train_loss:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}')\n",
    "        print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting warmup phase...\n",
      "Starting main training phase...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f860b4c213344ee9a52f47d5a292226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 0.3294\n",
      "Val Loss: 0.3497\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "598e3784c33949e7be71b6807af1b266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train Loss: 0.3285\n",
      "Val Loss: 0.3467\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508e4c9b00674ed69a81472241406727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train Loss: 0.3262\n",
      "Val Loss: 0.3295\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e48ac86b92b0475ab26e8f5aa02bd9de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train Loss: 0.0960\n",
      "Val Loss: 0.1071\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6cc2a9814149ba93351bc1e7832bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train Loss: 0.0802\n",
      "Val Loss: 0.1071\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2238506bbbca4e9cb3b66db962db94ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/10:   0%|          | 0/56 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_with_warmup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUNet_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munet.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_warm_up\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[98], line 50\u001b[0m, in \u001b[0;36mtrain_with_warmup\u001b[0;34m(model, train_loader, val_loader, num_epochs, filename, num_warm_up, learning_rate, patience, weight_decay)\u001b[0m\n\u001b[1;32m     48\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Gradient clipping\u001b[39;00m\n\u001b[1;32m     53\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opencv_env/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/opencv_env/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_with_warmup(\n",
    "    model=UNet_model,\n",
    "    train_loader=train_dataloader, \n",
    "    val_loader=val_dataloader,      \n",
    "    num_epochs=10,\n",
    "    filename = 'unet.pth',\n",
    "    num_warm_up=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bragehs/Documents/INF367A-DeforestationDrivers/models.py:211: FutureWarning: `nn.init.xavier_normal` is now deprecated in favor of `nn.init.xavier_normal_`.\n",
      "  init.xavier_normal(m.weight)\n",
      "/Users/bragehs/Documents/INF367A-DeforestationDrivers/models.py:212: FutureWarning: `nn.init.constant` is now deprecated in favor of `nn.init.constant_`.\n",
      "  init.constant(m.bias, 0)\n"
     ]
    }
   ],
   "source": [
    "model = UNet(in_channels=9, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(save_path, model_name):\n",
    "    # 1. Initialize model architecture\n",
    "    \n",
    "    # 2. Load the saved weights\n",
    "    weights_path = save_path + model_name\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    # 3. Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.split(os.getcwd())[0] + '/weights/'  # where to save model weights\n",
    "loaded_model = load_model(save_path, \"logdiceloss.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnhancedSatelliteSegmentationModel(\n",
      "  (conv1): Conv2d(12, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (enc1): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential()\n",
      "  )\n",
      "  (enc2): ResidualBlock(\n",
      "    (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc3): ResidualBlock(\n",
      "    (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (enc4): ResidualBlock(\n",
      "    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (shortcut): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (aspp): ASPP(\n",
      "    (aspp1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (aspp2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(6, 6), dilation=(6, 6), bias=False)\n",
      "    (aspp3): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
      "    (aspp4): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(18, 18), dilation=(18, 18), bias=False)\n",
      "    (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (conv_out): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (dec1): Sequential(\n",
      "    (0): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec2): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (dec3): Sequential(\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (skip3): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (final_up1): Sequential(\n",
      "    (0): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_up2): Sequential(\n",
      "    (0): ConvTranspose2d(32, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "  )\n",
      "  (final_conv): Sequential(\n",
      "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 5, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (dropout): Dropout2d(p=0.1, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_dataloader):\n",
    "    print(\"Starting prediction...\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions_flattened = []\n",
    "        targets = []\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_dataloader:\n",
    "                data = data.to(device)\n",
    "                target = target.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                pred = np.argmax(output.cpu().detach().numpy(), axis=1)\n",
    "                true = ((target.cpu().detach().numpy()).astype(int))\n",
    "                print('unique predictions:', np.unique(pred), 'unique labels:', np.unique(true))\n",
    "                predictions_flattened.extend(pred.flatten())\n",
    "                targets.extend(true.flatten())\n",
    "                predictions.extend(pred)\n",
    "    f1 = f1_score(predictions_flattened, targets, average='weighted')\n",
    "    print(\"F1 score = \", f1)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(predictions, targets):\n",
    "    f1 = f1_score(predictions, targets, average='weighted')\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(model, test_dataloader):\n",
    "    \"\"\"\n",
    "    Predicts the probabilities of each class for each pixel in the image.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        test_dataloader (torch.utils.data.DataLoader): The DataLoader for the test dataset.\n",
    "        device (str): The device to use for prediction (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        list: A list of predictions. Each element in the list corresponds to an image,\n",
    "              and contains 64 patches of predictions, each patch containing 128x128\n",
    "              probabilities for each class.\n",
    "    \"\"\"\n",
    "    print(\"Starting prediction...\")\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        predictions = []\n",
    "        for data, _ in test_dataloader:  # Changed to _ because target is not used\n",
    "            data = data.to(device)\n",
    "\n",
    "            output = model(data) # Assuming model returns a tuple, take the first element\n",
    "            probs = torch.nn.functional.softmax(output, dim=1).cpu().numpy()  # Convert to probabilities\n",
    "\n",
    "            # Iterate through the batch\n",
    "            batch_predictions = []\n",
    "            for batch_idx in range(data.shape[0]):\n",
    "                # Get coordinates of each pixel in the image\n",
    "                height, width = data.shape[2], data.shape[3]\n",
    "                x_coords, y_coords = np.meshgrid(np.arange(height), np.arange(width), indexing='ij')\n",
    "                x_coords = x_coords.flatten()\n",
    "                y_coords = y_coords.flatten()\n",
    "\n",
    "                # Combine coordinates and probabilities\n",
    "                image_predictions = []\n",
    "                for i in range(len(x_coords)):\n",
    "                    x = x_coords[i]\n",
    "                    y = y_coords[i]\n",
    "                    pixel_probs = probs[batch_idx, :, x, y].tolist()  # Extract probabilities for the pixel\n",
    "                    image_predictions.append((x, y, pixel_probs))  # Append as tuple\n",
    "                batch_predictions.append(image_predictions)\n",
    "            predictions.append(batch_predictions)\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Sentinel2SegmentationDataset(prepared_data[45:46], labels[45:46])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction...\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_probs(loaded_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataloader.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32, 16384)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions), len(predictions[0]), len(predictions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_patches(predictions, original_size=(1024, 1024), patch_size=128):\n",
    "    \"\"\"\n",
    "    Reconstructs a full image from patches with probability lists.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): A list of lists of lists of tuples. The outer list corresponds to images,\n",
    "                            the middle list corresponds to batches, and the inner list contains tuples\n",
    "                            of (x-coordinate, y-coordinate, probability list).\n",
    "        original_size (tuple): The size of the original image (height, width). Default is (1024, 1024).\n",
    "        patch_size (int): The size of the patches. Default is 128.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 3D numpy array representing the reconstructed image with shape (1024, 1024, 5).\n",
    "                       Each pixel contains a probability distribution over the 5 classes.\n",
    "    \"\"\"\n",
    "    # Initialize an empty array to hold the reconstructed image\n",
    "    full_img = np.zeros((original_size[0], original_size[1], 5))  # Shape: (1024, 1024, 5)\n",
    "\n",
    "    # Iterate through the images\n",
    "    for image_idx, image_predictions in enumerate(predictions):\n",
    "        # Iterate through the batches\n",
    "        for batch_idx, batch_patches in enumerate(image_predictions):\n",
    "            # Iterate through the patches and place them in the full image\n",
    "            for patch_data in batch_patches:\n",
    "                x, y, probs = patch_data  # Unpack the tuple\n",
    "\n",
    "                # Calculate the patch indices\n",
    "                patch_x_idx = (image_idx * len(image_predictions) + batch_idx) // (original_size[0] // patch_size)\n",
    "                patch_y_idx = (image_idx * len(image_predictions) + batch_idx) % (original_size[1] // patch_size)\n",
    "\n",
    "                # Calculate the actual pixel coordinates in the full image\n",
    "                pixel_x = x + patch_x_idx * patch_size\n",
    "                pixel_y = y + patch_y_idx * patch_size\n",
    "\n",
    "                # Place the probability list into the corresponding pixel in the full image\n",
    "                full_img[pixel_x, pixel_y, :] = probs  # Assign the probability list to the pixel\n",
    "\n",
    "    return full_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_image = reconstruct_from_patches(predictions, patch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[9.74279046e-01, 2.30785534e-02, 1.51482585e-03, 1.11663190e-03,\n",
       "         1.08987861e-05],\n",
       "        [9.99842763e-01, 7.07545041e-05, 3.69730551e-05, 4.84976990e-05,\n",
       "         9.61393425e-07],\n",
       "        [9.99945521e-01, 3.56434066e-05, 9.29395355e-06, 9.35443950e-06,\n",
       "         2.40309561e-07],\n",
       "        ...,\n",
       "        [9.90246415e-01, 7.59636424e-03, 2.14774907e-03, 8.98343478e-06,\n",
       "         4.56820743e-07],\n",
       "        [9.96146679e-01, 2.54894374e-03, 1.27080048e-03, 3.27818270e-05,\n",
       "         7.80794210e-07],\n",
       "        [6.80748463e-01, 2.62774557e-01, 5.54817766e-02, 9.61709418e-04,\n",
       "         3.35147524e-05]],\n",
       "\n",
       "       [[9.96183336e-01, 3.32768355e-03, 1.71671418e-04, 3.01081018e-04,\n",
       "         1.60954860e-05],\n",
       "        [9.99869347e-01, 1.03436883e-04, 1.25508186e-05, 1.12678872e-05,\n",
       "         3.30672719e-06],\n",
       "        [9.99831796e-01, 1.53968111e-04, 6.89702256e-06, 5.52792017e-06,\n",
       "         1.81949122e-06],\n",
       "        ...,\n",
       "        [9.60445285e-01, 3.78877409e-02, 1.66569138e-03, 9.08456570e-07,\n",
       "         3.59942703e-07],\n",
       "        [9.81090903e-01, 1.77645311e-02, 1.13829481e-03, 4.37931703e-06,\n",
       "         1.92634002e-06],\n",
       "        [7.91459739e-01, 1.71431780e-01, 3.67876962e-02, 2.75205908e-04,\n",
       "         4.55312183e-05]],\n",
       "\n",
       "       [[9.87596691e-01, 1.19494759e-02, 1.84511911e-04, 2.53549340e-04,\n",
       "         1.57650720e-05],\n",
       "        [9.99265254e-01, 7.02971942e-04, 1.60666368e-05, 1.19684855e-05,\n",
       "         3.82877397e-06],\n",
       "        [9.97159481e-01, 2.78179045e-03, 2.79095802e-05, 2.51517613e-05,\n",
       "         5.55793804e-06],\n",
       "        ...,\n",
       "        [7.73543477e-01, 2.25117743e-01, 1.33843243e-03, 1.83624564e-07,\n",
       "         1.97022572e-07],\n",
       "        [8.59105468e-01, 1.36720702e-01, 4.16670181e-03, 5.33461889e-06,\n",
       "         1.81265841e-06],\n",
       "        [5.31038284e-01, 4.24549341e-01, 4.42634486e-02, 1.17257870e-04,\n",
       "         3.16290716e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[4.18982527e-04, 9.97574866e-01, 1.90933782e-03, 9.30701572e-05,\n",
       "         3.70707949e-06],\n",
       "        [4.87082980e-05, 9.99780357e-01, 1.67612205e-04, 3.07556274e-06,\n",
       "         2.19715361e-07],\n",
       "        [2.86570639e-06, 9.99988198e-01, 8.51398181e-06, 5.19182379e-07,\n",
       "         2.17123173e-08],\n",
       "        ...,\n",
       "        [8.18768702e-03, 9.91604030e-01, 1.77145630e-04, 2.07300545e-05,\n",
       "         1.04316923e-05],\n",
       "        [1.40975967e-01, 8.57792914e-01, 1.09845388e-03, 9.46255677e-05,\n",
       "         3.81903519e-05],\n",
       "        [3.12790871e-01, 6.76162302e-01, 1.06735490e-02, 3.23657296e-04,\n",
       "         4.96607099e-05]],\n",
       "\n",
       "       [[1.49578333e-03, 9.85764563e-01, 1.24907382e-02, 2.36805921e-04,\n",
       "         1.20984214e-05],\n",
       "        [2.96630518e-04, 9.97503340e-01, 2.17959937e-03, 1.87742662e-05,\n",
       "         1.49284824e-06],\n",
       "        [1.39211770e-05, 9.99823630e-01, 1.59723189e-04, 2.64643404e-06,\n",
       "         1.10868378e-07],\n",
       "        ...,\n",
       "        [2.35390335e-01, 7.63262153e-01, 1.29761535e-03, 3.14456411e-05,\n",
       "         1.85137960e-05],\n",
       "        [8.08301330e-01, 1.88884005e-01, 2.69647618e-03, 9.61826881e-05,\n",
       "         2.20506554e-05],\n",
       "        [5.27708471e-01, 4.38094705e-01, 3.37569937e-02, 3.71513976e-04,\n",
       "         6.83201797e-05]],\n",
       "\n",
       "       [[4.97996341e-03, 9.74720001e-01, 1.90317836e-02, 1.23517029e-03,\n",
       "         3.31340598e-05],\n",
       "        [3.71003174e-03, 9.69350278e-01, 2.53848974e-02, 1.49583945e-03,\n",
       "         5.89583506e-05],\n",
       "        [3.09925759e-04, 9.92460310e-01, 6.83543179e-03, 3.80588026e-04,\n",
       "         1.36999270e-05],\n",
       "        ...,\n",
       "        [9.88896012e-01, 9.81724449e-03, 1.13226695e-03, 1.45839120e-04,\n",
       "         8.61640274e-06],\n",
       "        [9.88774419e-01, 9.07096826e-03, 1.91865175e-03, 2.23184004e-04,\n",
       "         1.27292897e-05],\n",
       "        [3.97533387e-01, 5.48981547e-01, 5.04587479e-02, 2.86532380e-03,\n",
       "         1.60999727e-04]]])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('reconstructed_image.pkl', 'wb') as f:\n",
    "    pickle.dump(reconstructed_image, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = labels[45:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024, 1024)"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rasterized image saved to rasterized_image.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "# Assuming y_test is a numpy array or torch tensor containing the\n",
    "# class labels for the entire 1024x1024 image.\n",
    "# Example:\n",
    "# y_test = ... # Your test labels (numpy array or torch tensor)\n",
    "\n",
    "# If y_test is a torch tensor, convert it to a numpy array\n",
    "if isinstance(y_test, torch.Tensor):\n",
    "    y_test = y_test.cpu().numpy()  # Move to CPU if it's on GPU\n",
    "\n",
    "# Ensure y_test is of integer type\n",
    "y_test = y_test.astype(np.uint8)\n",
    "\n",
    "# Create a color mapping for the classes\n",
    "color_map = {\n",
    "    0: [0, 0, 0],      # background: black\n",
    "    1: [255, 0, 0],    # plantation: red\n",
    "    2: [0, 255, 0],    # grassland_shrubland: green\n",
    "    3: [0, 0, 255],    # mining: blue\n",
    "    4: [255, 255, 0]     # logging: yellow\n",
    "}\n",
    "\n",
    "# Create an RGB image where each class is represented by its color\n",
    "height, width = y_test[0].shape\n",
    "rgb_image = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "for i in range(height):\n",
    "    for j in range(width):\n",
    "        class_id = y_test[0][i, j]\n",
    "        rgb_image[i, j] = color_map[class_id]\n",
    "\n",
    "# Create a PIL image from the numpy array\n",
    "img = Image.fromarray(rgb_image)\n",
    "\n",
    "# Save the image to a file\n",
    "img.save(\"rasterized_image.png\")\n",
    "\n",
    "print(\"Rasterized image saved to rasterized_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
