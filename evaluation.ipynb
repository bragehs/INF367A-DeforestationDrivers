{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from rasterio.features import rasterize\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE from solafune-tools\n",
    "class PixelBasedMetrics:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    def polygons_to_mask(self, polygons, array_dim):\n",
    "        \"\"\"\n",
    "        Converts a list of polygons into a binary mask.\n",
    "        \n",
    "        Args:\n",
    "            polygons (list): List of polygons, where each polygon is represented by a list of (x, y) tuples.\n",
    "            array_dim (tuple): Dimensions of the output mask (height, width).\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Binary mask with 1s for polygon areas and 0s elsewhere.\n",
    "        \"\"\"\n",
    "        shapes = [(polygon, 1) for polygon in polygons]\n",
    "        mask = rasterize(shapes, out_shape=array_dim, fill=0, dtype=np.uint8)\n",
    "        return mask\n",
    "    def compute_f1(self, gt_polygons, pred_polygons, array_dim=(1024, 1024)):\n",
    "        \"\"\"\n",
    "        Compute the F1 score, precision, and recall for the given ground truth and predicted polygons.\n",
    "        \n",
    "        Args:\n",
    "            gt_polygons (list): List of ground truth polygons.\n",
    "            pred_polygons (list): List of predicted polygons.\n",
    "            array_dim (tuple, optional): Dimensions of the output mask (height, width). Defaults to (1024, 1024).\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the F1 score, precision, and recall.\n",
    "        \"\"\"\n",
    "        # Pixel-level improvement\n",
    "        # Create binary masks for ground truth and predictions\n",
    "        gt_mask = self.polygons_to_mask(gt_polygons, array_dim)\n",
    "        pred_mask = self.polygons_to_mask(pred_polygons, array_dim)\n",
    "        \n",
    "        # Calculate pixel-level True Positives (TP), False Positives (FP), and False Negatives (FN)\n",
    "        tp = np.sum((gt_mask == 1) & (pred_mask == 1))\n",
    "        fp = np.sum((gt_mask == 0) & (pred_mask == 1))\n",
    "        fn = np.sum((gt_mask == 1) & (pred_mask == 0))\n",
    "        \n",
    "        # Calculate precision, recall, and F1 score\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 1.0  # if no prediction, precision is considered as 1\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 1.0  # if no ground truth, recall is considered as 1\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0  # if either precision or recall is 0, f1 is 0\n",
    "        \n",
    "        return f1, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05414926271340079\n"
     ]
    }
   ],
   "source": [
    "#Function which return the total F1 score for the training data\n",
    "def f1_scorer(path_ground, path_pred):\n",
    "    with open(f\"{path_ground}.json\", 'r') as file:\n",
    "        train_annotations = json.load(file)\n",
    "    with open(f\"{path_pred}.json\", 'r') as file:\n",
    "        pred_annotations = json.load(file)\n",
    "    score=[]\n",
    "    for i in range(len(train_annotations[\"images\"])):\n",
    "        gt_polygons = {'plantation' : [],\n",
    "        'grassland_shrubland': [],\n",
    "        'mining' :[],\n",
    "        'logging':[]}\n",
    "        pred_polygons = {'plantation' : [],\n",
    "        'grassland_shrubland': [],\n",
    "        'mining' :[],\n",
    "        'logging':[]}\n",
    "        for j in range(len(train_annotations[\"images\"][i][\"annotations\"])):\n",
    "            unproccesed_poly=train_annotations[\"images\"][i][\"annotations\"][j][\"segmentation\"]\n",
    "            processed_poly= Polygon(list(zip(unproccesed_poly[::2], unproccesed_poly[1::2])))\n",
    "            gt_polygons[train_annotations[\"images\"][i][\"annotations\"][j][\"class\"]].append(processed_poly)\n",
    "\n",
    "        for j in range(len(pred_annotations[\"images\"][i][\"annotations\"])):\n",
    "            unproccesed_poly=pred_annotations[\"images\"][i][\"annotations\"][j][\"segmentation\"]\n",
    "            processed_poly= Polygon(list(zip(unproccesed_poly[::2], unproccesed_poly[1::2])))\n",
    "            pred_polygons[pred_annotations[\"images\"][i][\"annotations\"][j][\"class\"]].append(processed_poly)\n",
    "        for key in gt_polygons.keys():\n",
    "\n",
    "            if len(gt_polygons[key]) == 0 and len(pred_polygons[key]) == 0:\n",
    "                #score.append(1)\n",
    "                continue\n",
    "            if len(gt_polygons[key]) == 0 or len(pred_polygons[key]) == 0:\n",
    "                score.append(0)\n",
    "                continue\n",
    "            f1, precision, recall = PixelBasedMetrics().compute_f1(gt_polygons[key], pred_polygons[key])\n",
    "            score.append(f1)\n",
    "        \n",
    "    return sum(score)/len(score)\n",
    "print(f1_scorer(\"train_annotations\",\"testy\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF265",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
