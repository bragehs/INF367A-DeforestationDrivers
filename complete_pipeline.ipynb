{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/satlaspretrain/lib/python3.9/site-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.4 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import PreProcessing\n",
    "import satlaspretrain_models\n",
    "from training import *\n",
    "from postprocessing import PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_manager = satlaspretrain_models.Weights()\n",
    "satlas_model = weights_manager.get_pretrained_model(\"Sentinel2_SwinT_SI_MS\", fpn=True, \n",
    "                                             head=satlaspretrain_models.Head.SEGMENT, \n",
    "                                                num_categories=5, device= \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "satlas_model = satlas_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model, save_path, model_name):\n",
    "    # 1. Initialize model architecture\n",
    "    \n",
    "    # 2. Load the saved weights\n",
    "    weights_path = save_path + model_name\n",
    "    state = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    #model.load_state_dict(state)\n",
    "    # 3. Set to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = load_model(satlas_model, '/Users/bragehs/Documents/weights/', \"best_combined_loss.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (backbone): SwinBackbone(\n",
      "    (backbone): SwinTransformer(\n",
      "      (features): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): Conv2d(9, 96, kernel_size=(4, 4), stride=(4, 4))\n",
      "          (1): Permute()\n",
      "          (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): Sequential(\n",
      "          (0): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
      "              (proj): Linear(in_features=96, out_features=96, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=3, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
      "            (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=96, out_features=384, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=384, out_features=96, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): PatchMergingV2(\n",
      "          (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
      "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): Sequential(\n",
      "          (0): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=6, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
      "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=192, out_features=768, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=768, out_features=192, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): PatchMergingV2(\n",
      "          (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (2): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (3): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (4): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (5): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=12, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
      "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=384, out_features=1536, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=1536, out_features=384, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): PatchMergingV2(\n",
      "          (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (1): SwinTransformerBlockV2(\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (attn): ShiftedWindowAttentionV2(\n",
      "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (cpb_mlp): Sequential(\n",
      "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
      "                (1): ReLU(inplace=True)\n",
      "                (2): Linear(in_features=512, out_features=24, bias=False)\n",
      "              )\n",
      "            )\n",
      "            (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (mlp): MLP(\n",
      "              (0): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (1): GELU(approximate='none')\n",
      "              (2): Dropout(p=0.0, inplace=False)\n",
      "              (3): Linear(in_features=3072, out_features=768, bias=True)\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (permute): Permute()\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (fpn): FPN(\n",
      "    (fpn): FeaturePyramidNetwork(\n",
      "      (inner_blocks): ModuleList(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (2): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (3): Conv2dNormActivation(\n",
      "          (0): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "      )\n",
      "      (layer_blocks): ModuleList(\n",
      "        (0-3): 4 x Conv2dNormActivation(\n",
      "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (upsample): Upsample(\n",
      "    (layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "        (3): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (head): SimpleHead(\n",
      "    (layers): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): Conv2d(128, 5, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(loaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing evaluation_0.tif\n",
      "Processing evaluation_1.tif\n",
      "Processing evaluation_2.tif\n",
      "Processing evaluation_3.tif\n",
      "Processing evaluation_4.tif\n",
      "Processing evaluation_5.tif\n",
      "Processing evaluation_6.tif\n",
      "Processing evaluation_7.tif\n",
      "Processing evaluation_8.tif\n",
      "Processing evaluation_9.tif\n",
      "Processing evaluation_10.tif\n",
      "Processing evaluation_11.tif\n",
      "Processing evaluation_12.tif\n",
      "Processing evaluation_13.tif\n",
      "Processing evaluation_14.tif\n",
      "Processing evaluation_15.tif\n",
      "Processing evaluation_16.tif\n",
      "Processing evaluation_17.tif\n",
      "Processing evaluation_18.tif\n",
      "Processing evaluation_19.tif\n",
      "Processing evaluation_20.tif\n",
      "Processing evaluation_21.tif\n",
      "Processing evaluation_22.tif\n",
      "Processing evaluation_23.tif\n",
      "Processing evaluation_24.tif\n",
      "Processing evaluation_25.tif\n",
      "Processing evaluation_26.tif\n",
      "Processing evaluation_27.tif\n",
      "Processing evaluation_28.tif\n",
      "Processing evaluation_29.tif\n",
      "Processing evaluation_30.tif\n",
      "Processing evaluation_31.tif\n",
      "Processing evaluation_32.tif\n",
      "Processing evaluation_33.tif\n",
      "Processing evaluation_34.tif\n",
      "Processing evaluation_35.tif\n",
      "Processing evaluation_36.tif\n",
      "Processing evaluation_37.tif\n",
      "Processing evaluation_38.tif\n",
      "Processing evaluation_39.tif\n",
      "Processing evaluation_40.tif\n",
      "Processing evaluation_41.tif\n",
      "Processing evaluation_42.tif\n",
      "Processing evaluation_43.tif\n",
      "Processing evaluation_44.tif\n",
      "Processing evaluation_45.tif\n",
      "Processing evaluation_46.tif\n",
      "Processing evaluation_47.tif\n",
      "Processing evaluation_48.tif\n",
      "Processing evaluation_49.tif\n",
      "Processing evaluation_50.tif\n",
      "Processing evaluation_51.tif\n",
      "Processing evaluation_52.tif\n",
      "Processing evaluation_53.tif\n",
      "Processing evaluation_54.tif\n",
      "Processing evaluation_55.tif\n",
      "Processing evaluation_56.tif\n",
      "Processing evaluation_57.tif\n",
      "Processing evaluation_58.tif\n",
      "Processing evaluation_59.tif\n",
      "Processing evaluation_60.tif\n",
      "Processing evaluation_61.tif\n",
      "Processing evaluation_62.tif\n",
      "Processing evaluation_63.tif\n",
      "Processing evaluation_64.tif\n",
      "Processing evaluation_65.tif\n",
      "Processing evaluation_66.tif\n",
      "Processing evaluation_67.tif\n",
      "Processing evaluation_68.tif\n",
      "Processing evaluation_69.tif\n",
      "Processing evaluation_70.tif\n",
      "Processing evaluation_71.tif\n",
      "Processing evaluation_72.tif\n",
      "Processing evaluation_73.tif\n",
      "Processing evaluation_74.tif\n",
      "Processing evaluation_75.tif\n",
      "Processing evaluation_76.tif\n",
      "Processing evaluation_77.tif\n",
      "Processing evaluation_78.tif\n",
      "Processing evaluation_79.tif\n",
      "Processing evaluation_80.tif\n",
      "Processing evaluation_81.tif\n",
      "Processing evaluation_82.tif\n",
      "Processing evaluation_83.tif\n",
      "Processing evaluation_84.tif\n",
      "Processing evaluation_85.tif\n",
      "Processing evaluation_86.tif\n",
      "Processing evaluation_87.tif\n",
      "Processing evaluation_88.tif\n",
      "Processing evaluation_89.tif\n",
      "Processing evaluation_90.tif\n",
      "Processing evaluation_91.tif\n",
      "Processing evaluation_92.tif\n",
      "Processing evaluation_93.tif\n",
      "Processing evaluation_94.tif\n",
      "Processing evaluation_95.tif\n",
      "Processing evaluation_96.tif\n",
      "Processing evaluation_97.tif\n",
      "Processing evaluation_98.tif\n",
      "Processing evaluation_99.tif\n",
      "Processing evaluation_100.tif\n",
      "Processing evaluation_101.tif\n",
      "Processing evaluation_102.tif\n",
      "Processing evaluation_103.tif\n",
      "Processing evaluation_104.tif\n",
      "Processing evaluation_105.tif\n",
      "Processing evaluation_106.tif\n",
      "Processing evaluation_107.tif\n",
      "Processing evaluation_108.tif\n",
      "Processing evaluation_109.tif\n",
      "Processing evaluation_110.tif\n",
      "Processing evaluation_111.tif\n",
      "Processing evaluation_112.tif\n",
      "Processing evaluation_113.tif\n",
      "Processing evaluation_114.tif\n",
      "Processing evaluation_115.tif\n",
      "Processing evaluation_116.tif\n",
      "Processing evaluation_117.tif\n",
      "self.prepared_data.shape: (118, 9, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "test_data = PreProcessing(train_set=False)\n",
    "test_data.preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(118, 9, 1024, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data.prepared_data\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "post = PostProcessing(loaded_model, test_data[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post_opt = post.post_process_optimized(patch_size=64)\n",
    "#polygons = post.converter(post_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    \"\"\"Dataset class for test data without labels\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        images: np.ndarray,  # Shape: (N, C, H, W)\n",
    "        patch_size: int = 256,\n",
    "        stride: int = 256\n",
    "    ):\n",
    "        self.images = images\n",
    "        self.patch_size = patch_size\n",
    "        self.stride = stride\n",
    "        self.patches, self.positions = self._create_patches()\n",
    "\n",
    "    def _create_patches(self):\n",
    "        \"\"\"Create patches and store their original positions\"\"\"\n",
    "        patches = []\n",
    "        positions = []  # Store (image_idx, y, x) for each patch\n",
    "        N, C, H, W = self.images.shape\n",
    "\n",
    "        for img_idx in range(N):\n",
    "            image = self.images[img_idx]\n",
    "            for y in range(0, H - self.patch_size + 1, self.stride):\n",
    "                for x in range(0, W - self.patch_size + 1, self.stride):\n",
    "                    img_patch = image[:, y:y + self.patch_size, x:x + self.patch_size]\n",
    "                    patches.append(img_patch)\n",
    "                    positions.append((img_idx, y, x))\n",
    "\n",
    "        return patches, positions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.as_tensor(self.patches[idx], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TestDataset(\n",
    "    images=test_data[:2],\n",
    "    patch_size=128,\n",
    "    stride=128\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "                            test_dataset, \n",
    "                            batch_size=64, \n",
    "                            shuffle=False, \n",
    "                            num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probs(model, test_dataloader):\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            data = data.to(device)\n",
    "            output = model(data)[0]  # Shape: (batch_size, 5, 128, 128)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            # Permute to (batch_size, 128, 128, 5) and collect\n",
    "            probs = probs.permute(0, 2, 3, 1).cpu().numpy()\n",
    "            all_probs.append(probs)\n",
    "    # Concatenate all batches into (num_patches, 128, 128, 5)\n",
    "    return np.concatenate(all_probs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = predict_probs(loaded_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stitch_patches(prob_patches, positions, image_shape=(1024, 1024), patch_size=128):\n",
    "    num_classes = prob_patches.shape[-1]\n",
    "    stitched_images = []\n",
    "    # Extract unique image indices from positions\n",
    "    image_indices = sorted(set(pos[0] for pos in positions))\n",
    "    for img_idx in image_indices:\n",
    "        full_image = np.zeros((image_shape[0], image_shape[1], num_classes))\n",
    "        # Iterate through all patches and place them if they belong to the current image\n",
    "        for i, (curr_idx, y, x) in enumerate(positions):\n",
    "            if curr_idx == img_idx:\n",
    "                full_image[y:y+patch_size, x:x+patch_size, :] = prob_patches[i]\n",
    "        stitched_images.append(full_image)\n",
    "    return [torch.tensor(img) for img in stitched_images]\n",
    "\n",
    "\n",
    "def post_process_torch(outputs, gamma=0.5):\n",
    "    all_images = []\n",
    "    for img in outputs:  # Each img is a tensor of shape (1024, 1024, 5)\n",
    "        # Pad the image to handle borders\n",
    "        padded = torch.nn.functional.pad(img, (0, 0, 1, 1, 1, 1))  # Pad H and W by 1\n",
    "        \n",
    "        # Compute contributions from 4-directional neighbors\n",
    "        top = padded[:-2, 1:-1, :]    # Shape: (1024, 1024, 5)\n",
    "        bottom = padded[2:, 1:-1, :]\n",
    "        left = padded[1:-1, :-2, :]\n",
    "        right = padded[1:-1, 2:, :]\n",
    "        \n",
    "        # Combine neighbor contributions and add to original probabilities\n",
    "        neighbors = gamma * (top + bottom + left + right)\n",
    "        combined_probs = img + neighbors\n",
    "        \n",
    "        # Get final predictions\n",
    "        class_predictions = torch.argmax(combined_probs, dim=2)\n",
    "        all_images.append(class_predictions)\n",
    "    \n",
    "    return all_images  # List of (1024, 1024) tensors with class indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = test_dataloader.dataset.positions  # Get positions from dataset\n",
    "stitched_tensors = stitch_patches(preds, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2092, 0.1700, 0.2911, 0.1693, 0.1603], dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stitched_tensors[1][0][1023]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_processed = post_process_torch(stitched_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter(tensors):\n",
    "    \"\"\"\n",
    "    Convert multiple image tensors to polygons.\n",
    "    \n",
    "    Args:\n",
    "        tensors: List of tensors or single tensor. Each tensor should be (1024, 1024) \n",
    "                containing class labels 0-4 (either numpy array or torch.Tensor)\n",
    "    \n",
    "    Returns:\n",
    "        list[dict]: List of dictionaries, one per image, where each dictionary\n",
    "                contains polygons for each class {0: [...], 1: [...], ...}\n",
    "    \"\"\"\n",
    "    # Handle single tensor case and convert to numpy\n",
    "    if isinstance(tensors, (np.ndarray, torch.Tensor)):\n",
    "        tensors = [tensors]\n",
    "    \n",
    "    all_image_polygons = []\n",
    "    \n",
    "    for idx, tensor in enumerate(tensors):\n",
    "        print(\"Converting tensor to polygons for image\", idx)\n",
    "        image_polygons = {}\n",
    "        \n",
    "        # Convert to numpy and ensure proper 2D format\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            # Handle both CPU and GPU tensors\n",
    "            tensor_np = tensor.cpu().detach().numpy().squeeze().astype(np.uint8)\n",
    "            print(tensor_np.shape)\n",
    "        else:\n",
    "            tensor_np = tensor.squeeze().astype(np.uint8)\n",
    "        \n",
    "        # Critical validation\n",
    "        if tensor_np.ndim != 2:\n",
    "            raise ValueError(f\"Input tensor must be 2D after squeezing. Got {tensor_np.shape}\")\n",
    "        \n",
    "        for class_id in range(5):\n",
    "            # Create binary mask\n",
    "            mask = (tensor_np == class_id).astype(np.uint8)\n",
    "            \n",
    "            # Verify OpenCV requirements\n",
    "            if not isinstance(mask, np.ndarray):\n",
    "                raise TypeError(f\"Mask must be numpy array, got {type(mask)}\")\n",
    "            if mask.dtype != np.uint8:\n",
    "                mask = mask.astype(np.uint8)\n",
    "            \n",
    "            # Find contours\n",
    "            contours, _ = cv2.findContours(\n",
    "                mask, \n",
    "                cv2.RETR_EXTERNAL, \n",
    "                cv2.CHAIN_APPROX_SIMPLE\n",
    "            )\n",
    "            \n",
    "            # Convert and filter contours\n",
    "            image_polygons[class_id] = [\n",
    "                contour.squeeze().tolist() \n",
    "                for contour in contours \n",
    "                if contour.shape[0] >= 3  # Minimum 3 points for polygon\n",
    "            ]\n",
    "        \n",
    "        all_image_polygons.append(image_polygons)\n",
    "    \n",
    "    return all_image_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting tensor to polygons for image 0\n",
      "(1024, 1024)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Mask must be numpy array, got <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m polygons \u001b[38;5;241m=\u001b[39m \u001b[43mconverter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpost_processed\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[190], line 41\u001b[0m, in \u001b[0;36mconverter\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Verify OpenCV requirements\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMask must be numpy array, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(mask)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m np\u001b[38;5;241m.\u001b[39muint8:\n\u001b[1;32m     43\u001b[0m     mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mTypeError\u001b[0m: Mask must be numpy array, got <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "polygons = converter(post_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "satlaspretrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
